{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "I am not even going to try and write a better intro. to neural nets than this...\n",
    "\n",
    "https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/\n",
    "\n",
    "### Softmax Equation\n",
    "\n",
    "Given an array of values of length n, the softmax of value i in the array is:\n",
    "\n",
    "$$\\frac{e^{i}}{\\sum_{j}^{n}e^{j}}$$\n",
    "\n",
    "### Deep Neural Network\n",
    "\n",
    "When you have multiple hidden layers - the layers in between the input and softmax layers, the network is called deep.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Neural nets are trained using a technique called backpropagation. At a very high level, you pass a training example through your network (forward pass), then measure its error, and then you go backwards through each layer to measure the contribution of each connection to the error (backwards pass). You then use this information to adjust the weights of your connections using gradient descent. \n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "The article above does not talk much about activation functions. Typically, in an MLP after you pass connections to a neuron you then apply an activation function. Historically, that activation function was a logistic function, which then is basically logistic regression.\n",
    "\n",
    "Another very popular activation function now is relu. Relu(z) = max(0,z). This is very fast to compute and in practice works very well.\n",
    "\n",
    "### Cross-entropy\n",
    "\n",
    "$$-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{i}log(p_{k}^{i})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* m - the number of data points\n",
    "* K - the number of classes\n",
    "* y_{k}^{i} - the true class value for row i, class k. Either a zero or one depending on if k is the correct class\n",
    "* p_{k}^{i} - the value predicted by your model for class k, row i. Usually from your softmax\n",
    "\n",
    "This is the cost function you are trying to minimze.\n",
    "\n",
    "### Important to Remember\n",
    "\n",
    "* Scale data - usually zero to one\n",
    "* Shuffle data\n",
    "\n",
    "### Tuning Hyper-parameters\n",
    "\n",
    "* Better to use random search\n",
    "* Start with reasonable, known architectures\n",
    "* Number of hidden layers:\n",
    "    * Often can be valuable to have a deep network to learn heirarchy. Usually converge faster and generalize better. \n",
    "    * More complex problems can often require deeper networks and more data\n",
    "* Number of neurons:\n",
    "    * Typically size the layers to form a type of funnel with fewer and fewer neurons at each layer. This comes back the heirachy idea where you might need more neurons to learn lower level features. \n",
    "    * Also can try picking same number of neurons for all layers to have less parameters to tune\n",
    "* Usually more value in going deeper than wider\n",
    "* Can try going deeper and wider than you think necessary and use regularization techinques to prevent overfitting. Such as early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.02, 0.0, 0.97999999999999998]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values = np.array([1.0, 3.0, 8.0, 4.0, 12.0])\n",
    "exp_values = np.exp(values)\n",
    "softmax = exp_values / sum(exp_values)\n",
    "print([round(x,2) for x in softmax])\n",
    "print(sum(softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "def vectorize_image(images):\n",
    "    scaled_images = images / 255\n",
    "    return images.reshape(scaled_images.shape[0],-1)\n",
    "\n",
    "x_train = vectorize_image(x_train)\n",
    "x_test = vectorize_image(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_shape=(784,)),\n",
    "    Dense(10, activation='relu'),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s - loss: 13.1458     9\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s - loss: 11.9375     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s - loss: 11.0628     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s - loss: 11.1027     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s - loss: 10.2141     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127645e48>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model.predict(x_test),1)\n",
    "y_test_sparse = np.argmax(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[865,   0,   0,   2,   0,  35,   0,  70,   0,   8],\n",
       "       [  0,   0,   0, 869,   0,  95,   0,   0,   0, 171],\n",
       "       [ 74,   0,   0, 702,   0,  36,   0, 118,   0, 102],\n",
       "       [ 12,   0,   0, 846,   0,  73,   0,  48,   0,  31],\n",
       "       [  3,   0,   0,   2,   0,   7,   0,  12,   0, 958],\n",
       "       [ 32,   0,   0,  60,   0, 694,   0,  53,   0,  53],\n",
       "       [ 61,   0,   0,  11,   0, 389,   0,  34,   0, 463],\n",
       "       [  0,   0,   0,  56,   0,   0,   0, 774,   0, 198],\n",
       "       [ 27,   0,   0, 232,   0, 457,   0,  66,   0, 192],\n",
       "       [  3,   0,   0,  13,   0,  11,   0,  23,   0, 959]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_sparse, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4138])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_sparse == test_predictions) / test_predictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
