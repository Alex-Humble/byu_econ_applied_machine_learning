{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "I am not even going to try and write a better intro. to neural nets than this...\n",
    "\n",
    "https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/\n",
    "\n",
    "### Softmax Equation\n",
    "\n",
    "Given an array of values of length n, the softmax of value i in the array is:\n",
    "\n",
    "$$\\frac{e^{i}}{\\sum_{j}^{n}e^{j}}$$\n",
    "\n",
    "### Deep Neural Network\n",
    "\n",
    "When you have multiple hidden layers - the layers in between the input and softmax layers, the network is called deep.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Neural nets are trained using a technique called backpropagation. At a very high level, you pass a training example through your network (forward pass), then measure its error, and then you go backwards through each layer to measure the contribution of each connection to the error (backwards pass). You then use this information to adjust the weights of your connections using gradient descent. \n",
    "\n",
    "### Vanishing/Exploding gradients\n",
    "\n",
    "When your gradients start to get too small or too large this can negatively effect learning. For example, a zero gradient will stop learning all together and when you gradients get too large your learning can diverge.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "The article above does not talk much about activation functions. Typically, in an MLP after you pass connections to a neuron you then apply an activation function. Historically, that activation function was a logistic function, which then is basically logistic regression. This tends to suffer from vanishing gradient problem.\n",
    "\n",
    "Another very popular activation function now is relu. Relu(z) = max(0,z). This is very fast to compute and in practice works very well. This function suffers less from the vanishing gradient problem.\n",
    "\n",
    "One problem with relu is that the connections can die. This happens if the inputs to a neuron end up negative resulting in a zero gradient. Thus, the **leaky relu** was invented: Leaky Relu(x) = max($\\alpha$x, x) where $\\alpha$ is usually a value of 0.01 or 0.02. The $\\alpha$ value is the slope when x < 0 and ensures that the activation never truly dies, though it can become quite small.\n",
    "\n",
    "**Elu** is another activation function which generally performs the best but is slower to compute then a leaky relu. Again, when x > 0 you just get x. But when x < 0 you get $\\alpha$(exp(x) -1). $\\alpha$ represents the value that the function approaches when x is a large negative number. Usually, it is set to 1. This function is also smooth everywhere, including zero.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "As we have learned it is important to scale - or normalize - your data before feeding it to a neural net. Another important normalization step is right before your activation function to again normalize your data by subtracting the mean and dividing by the standard deviation. Since you are working with a batch, you use the batch mean and standard deviation. You also allow each batch normalization to learn an appropriate scaling and shifiting factor for your standardized values. \n",
    "\n",
    "This technique has been shown to reduce the vanishing/exploding gradient problem, allow the use or larger learning rates, and be less sensitive to initalization. On the downside, it reduces runtime prediction speed.\n",
    "\n",
    "\n",
    "### Cross-entropy\n",
    "\n",
    "$$-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{i}log(p_{k}^{i})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* m - the number of data points\n",
    "* K - the number of classes\n",
    "* y_{k}^{i} - the true class value for row i, class k. Either a zero or one depending on if k is the correct class\n",
    "* p_{k}^{i} - the value predicted by your model for class k, row i. Usually from your softmax\n",
    "\n",
    "This is the cost function you are trying to minimze.\n",
    "\n",
    "### Important to Remember\n",
    "\n",
    "* Scale data - usually zero to one\n",
    "* Shuffle data\n",
    "\n",
    "### Tuning Hyper-parameters\n",
    "\n",
    "* Better to use random search\n",
    "* Start with reasonable, known architectures\n",
    "* Number of hidden layers:\n",
    "    * Often can be valuable to have a deep network to learn heirarchy. Usually converge faster and generalize better. \n",
    "    * More complex problems can often require deeper networks and more data\n",
    "* Number of neurons:\n",
    "    * Typically size the layers to form a type of funnel with fewer and fewer neurons at each layer. This comes back the heirachy idea where you might need more neurons to learn lower level features. \n",
    "    * Also can try picking same number of neurons for all layers to have less parameters to tune\n",
    "* Usually more value in going deeper than wider\n",
    "* Can try going deeper and wider than you think necessary and use regularization techinques to prevent overfitting. Such as early stopping.\n",
    "\n",
    "### Pretraining\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "### Initialization\n",
    "\n",
    "### Regularization\n",
    "\n",
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.02, 0.0, 0.97999999999999998]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values = np.array([1.0, 3.0, 8.0, 4.0, 12.0])\n",
    "exp_values = np.exp(values)\n",
    "softmax = exp_values / sum(exp_values)\n",
    "print([round(x,2) for x in softmax])\n",
    "print(sum(softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "def vectorize_image(images):\n",
    "    scaled_images = images / 255\n",
    "    return images.reshape(scaled_images.shape[0],-1)\n",
    "\n",
    "x_train = vectorize_image(x_train)\n",
    "x_test = vectorize_image(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_shape=(784,), activation='relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s - loss: 9.8007     21\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s - loss: 7.1868     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s - loss: 7.0780     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s - loss: 6.9717     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s - loss: 6.6994     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1281ebeb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model.predict(x_test),1)\n",
    "y_test_sparse = np.argmax(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,   34,  265,    9,  452,  213,    7,    0,    0],\n",
       "       [   0, 1120,    3,    6,    0,    2,    4,    0,    0,    0],\n",
       "       [   0,    1,  954,   34,   12,    7,   14,   10,    0,    0],\n",
       "       [   0,    2,   80,  858,    2,   44,    8,   16,    0,    0],\n",
       "       [   0,    1,    6,    5,  953,    3,   13,    1,    0,    0],\n",
       "       [   0,    0,    2,   63,   23,  785,   15,    4,    0,    0],\n",
       "       [   0,    3,    2,    4,    4,   14,  931,    0,    0,    0],\n",
       "       [   0,   11,   13,   60,   17,    1,    2,  924,    0,    0],\n",
       "       [   0,   21,   51,  562,  179,  113,   20,   28,    0,    0],\n",
       "       [   0,   12,    0,   32,  817,   19,    4,  125,    0,    0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_sparse, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6525])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_sparse == test_predictions) / test_predictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
