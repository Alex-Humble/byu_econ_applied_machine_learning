{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks - or RNNs - are built specifically to deal with sequence data. For example, suppose you have a sequence of text of movie reviews and would like to classify their sentiment, or a sequence of stock prices and you would like to predict the next one. These are all tasks well suited for an RNN.\n",
    "\n",
    "To better understand, let's take a look at this blog post:\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "RNN's can take many different forms:\n",
    "\n",
    "* Sequence of inputs to sequence of outputs\n",
    "* Sequence of inputs to vector of output\n",
    "* Vector of input to sequence of outps\n",
    "* Encoder -> Decoder\n",
    "\n",
    "We can take a closer look on p. 384 of Hands on Machine Learning\n",
    "\n",
    "## Variable length sequences\n",
    "\n",
    "If you have variable length inputs, like movie reviews which differ in length. A decent technique is to pick a fairly large input sequence length and zero padd all the inputs which are smaller. See here:\n",
    "\n",
    "https://github.com/keras-team/keras/issues/40\n",
    "\n",
    "If you have variable length output sequences - for example, when generating text. You can define a special end of sequence tag such as <EOS> and ignore any output past that tag.\n",
    "\n",
    "\n",
    "## Issues with RNNs:\n",
    "\n",
    "* Vanishing/Exploding gradients\n",
    "* Take a long time to train\n",
    "* Memory of first inputs tends to fad away making their long-term memory weak\n",
    "\n",
    "## LSTM\n",
    "\n",
    "* Much more capable of learning long-term dependencies\n",
    "* Same structure as vanilla RNN, but has 4 neural networks inside\n",
    "* Has two states which are passed to the next part of the sequence\n",
    "* The first state is the **cell state** and only has small changes made to it and thus it is very easy for information to be passed forward.\n",
    "* The first change we can make to the cell state is remove information from it. This is done with a **forget gate.** The forget gate is a fully connected neural net where the input is the concat of previous hidden state (not the cell state) and the sequence input. It is activated with a sigmoid and thus gives values from zero to one. A 1 means keep all the information.\n",
    "* The second change we make to the cell state is decide which information should be added. To do this, we take the combined hidden and input and feed it through another fully connected layer activated by a sigmoid - this is the **input gate.** We also take this combined input and feed it through what I will call the **tanh gate.** This gate is a fully connected layer activated by a tanh function - like sigmoid but ranges from -1 to 1. We then combine these two gates output with pointwise multiplication - basically the input gate tells how much information to keep from the tanh gate.\n",
    "* Now that we have our forget and remember values we will take the incoming cell state and multiply it by the forget values and add in the remember values.\n",
    "* Lastly, we need to decide what to output which will also be our hidden state we carry forward.\n",
    "* First, we take the combined input and hidden state through what I will call the **output gate.** This is a fully connected layer activated by a sigmoid.\n",
    "* Then, we take the updated cell value and pass it through a tanh to force its values to range from -1 to 1 and multiply these values by the output gate - the output gate telling us what to keep.\n",
    "* We carry this value forward as the hidden state and pass it through a softmax and a final fully connected layer to get our output.\n",
    "\n",
    "## GRU\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "## Encoder-Decoder\n",
    "\n",
    "## Example of RNN using pytorch\n",
    "\n",
    "Source: http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#sphx-glr-intermediate-char-rnn-generation-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import torch.distributions as distributions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_names = []\n",
    "name_weights = []\n",
    "for year in range(2000,2017):\n",
    "    with open(\"../small_data/baby_names/yob{}.txt\".format(year), \"r\") as f:\n",
    "        for line in f:\n",
    "            columns = line.split(\",\")\n",
    "            if columns[1] == 'F':\n",
    "                female_names.append(columns[0])\n",
    "                name_weights.append(int(columns[2].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emily', 'Hannah', 'Madison', 'Ashley', 'Sarah']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326418\n"
     ]
    }
   ],
   "source": [
    "n_names = len(female_names)\n",
    "print(n_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25953, 23078, 19967, 17996, 17691]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names_weights = defaultdict(int)\n",
    "for i in range(n_names):\n",
    "    name = female_names[i]\n",
    "    weight = name_weights[i]\n",
    "    unique_names_weights[name] = unique_names_weights[name] + weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = list(unique_names_weights.keys())\n",
    "unique_weights = list(unique_names_weights.values())\n",
    "unique_probabilities = np.array(unique_weights) / sum(unique_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.result_gate = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        \n",
    "        forget_gate_value = self.sigmoid(self.forget_gate(input_combined))\n",
    "        input_gate_value = self.sigmoid(self.input_gate(input_combined))\n",
    "        output_gate_value = self.sigmoid(self.output_gate(input_combined))\n",
    "        tanh_gate_value = self.tanh(self.tanh_gate(input_combined))\n",
    "        \n",
    "        input_tanh_combined = input_gate_value * tanh_gate_value\n",
    "        \n",
    "        cell_next = cell * forget_gate_value + input_tanh_combined\n",
    "        hidden_next = self.tanh(cell_next) * output_gate_value\n",
    "        output = self.result_gate(hidden_next)\n",
    "\n",
    "        return output, hidden_next, cell_next\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = set([c.lower() for name in female_names for c in name])\n",
    "letters_to_index = {l:i for i,l in enumerate(letters)}\n",
    "index_to_letter = {i:l for l, i in letters_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letters) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li].lower()\n",
    "        tensor[li][0][letters_to_index[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [letters_to_index[line[li].lower()] for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def randomTrainingPair():\n",
    "    return np.random.choice(unique_names, p=unique_probabilities, size=1)[0]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    line = randomTrainingPair()\n",
    "    input_line_tensor = Variable(inputTensor(line))\n",
    "    target_line_tensor = Variable(targetTensor(line))\n",
    "    return input_line_tensor, target_line_tensor\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lstm = LSTM(n_letters, 128, n_letters)\n",
    "\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=.001)\n",
    "\n",
    "def train(input_line_tensor, target_line_tensor):\n",
    "    hidden = lstm.initHidden()\n",
    "    cell = lstm.initHidden()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size()[0]):\n",
    "        output, hidden, cell = lstm(input_line_tensor[i], hidden, cell)\n",
    "        loss += criterion(output, target_line_tensor[i])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0] / input_line_tensor.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 54s (5000 25%) 2.0274\n",
      "1m 52s (10000 50%) 1.2021\n",
      "2m 48s (15000 75%) 0.6805\n",
      "6m 26s (20000 100%) 3.5369\n"
     ]
    }
   ],
   "source": [
    "n_iters = 20000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1221c22e8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRURCkMpdRCQEqdxFREKQyl1EJAT9fwTcKjsF/TANAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121a517f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter='A'):\n",
    "    input = Variable(inputTensor(start_letter))\n",
    "    hidden = lstm.initHidden()\n",
    "    cell = lstm.initHidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output, hidden, cell = lstm(input[0], hidden, cell)\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        softmax_out = sm(Variable(output.data)).data.numpy()[0]\n",
    "        topi = np.random.choice(range(len(softmax_out)), p=softmax_out)\n",
    "        if topi == n_letters - 1:\n",
    "            break\n",
    "        else:\n",
    "            letter = index_to_letter[topi]\n",
    "            output_name += letter\n",
    "        input = Variable(inputTensor(letter))\n",
    "\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angele'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
