{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks - or RNNs - are built specifically to deal with sequence data. For example, suppose you have a sequence of text of movie reviews and would like to classify their sentiment, or a sequence of stock prices and you would like to predict the next one. These are all tasks well suited for an RNN.\n",
    "\n",
    "To better understand, let's take a look at this blog post:\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "RNN's can take many different forms:\n",
    "\n",
    "* Sequence of inputs to sequence of outputs\n",
    "* Sequence of inputs to vector of output\n",
    "* Vector of input to sequence of outps\n",
    "* Encoder -> Decoder\n",
    "\n",
    "We can take a closer look on p. 384 of Hands on Machine Learning\n",
    "\n",
    "## Variable length sequences\n",
    "\n",
    "If you have variable length inputs, like movie reviews which differ in length. A decent technique is to pick a fairly large input sequence length and zero padd all the inputs which are smaller. See here:\n",
    "\n",
    "https://github.com/keras-team/keras/issues/40\n",
    "\n",
    "If you have variable length output sequences - for example, when generating text. You can define a special end of sequence tag such as <EOS> and ignore any output past that tag.\n",
    "\n",
    "\n",
    "## Issues with RNNs:\n",
    "\n",
    "* Vanishing/Exploding gradients\n",
    "* Take a long time to train\n",
    "* Memory of first inputs tends to fad away making their long-term memory weak\n",
    "\n",
    "## LSTM\n",
    "\n",
    "* Much more capable of learning long-term dependencies\n",
    "* Same structure as vanilla RNN, but has 4 neural networks inside\n",
    "* Has two states which are passed to the next part of the sequence\n",
    "* The first state is the **cell state** and only has small changes made to it and thus it is very easy for information to be passed forward.\n",
    "* The first change we can make to the cell state is remove information from it. This is done with a **forget gate.** The forget gate is a fully connected neural net where the input is the concat of previous hidden state (not the cell state) and the sequence input. It is activated with a sigmoid and thus gives values from zero to one. A 1 means keep all the information.\n",
    "* The second change we make to the cell state is decide which information should be added. To do this, we take the combined hidden and input and feed it through another fully connected layer activated by a sigmoid - this is the **input gate.** We also take this combined input and feed it through what I will call the **tanh gate.** This gate is a fully connected layer activated by a tanh function - like sigmoid but ranges from -1 to 1. We then combine these two gates output with pointwise multiplication - basically the input gate tells how much information to keep from the tanh gate.\n",
    "* Now that we have our forget and remember values we will take the incoming cell state and multiply it by the forget values and add in the remember values.\n",
    "* Lastly, we need to decide what to output which will also be our hidden state we carry forward.\n",
    "* First, we take the combined input and hidden state through what I will call the **output gate.** This is a fully connected layer activated by a sigmoid.\n",
    "* Then, we take the updated cell value and pass it through a tanh to force its values to range from -1 to 1 and multiply these values by the output gate - the output gate telling us what to keep.\n",
    "* We carry this value forward as the hidden state and pass it through a softmax and a final fully connected layer to get our output.\n",
    "\n",
    "## GRU\n",
    "\n",
    "GRU is basically a simplified version of an LSTM that seems to perform just as well for most tasks and thus has been growing in popularity. To learn more about it take a look at the blog post or p. 406 of Hands on Machine learning.\n",
    "\n",
    "There are also many other RNN architectures that we don't metion here.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "A very interesting challenge in NLP is: how do we represent the meaning of a word to a computer? One idea might be to reprsent words as a sparse vector the size of your vocabulary where each vector is all zeros except for a one at the appropriate index for that word. This has some pretty obvious drawbacks. One, it is very large. Two, is it really has no notion of how words are similar in their meanings.\n",
    "\n",
    "One way we have solved some portion of these problems is by learning dense word embeddings. These embeddings - or vectors - should encode the meaning of a word sufficiently enough that we can hopefully compare vectors.\n",
    "\n",
    "A technique to do this - there are many - is called **Continuous Bag-of-Words (CBOW)**. Also, referred to as word2vec, though, there is another flavor of word2vec. The idea is to use the context of a word as features to predict the target word.\n",
    "\n",
    "What is the context? Simple - the words surrounding the target word. Usually, around +-2 words. You then define how big of a dimension you want for your embeddings - say 128.\n",
    "\n",
    "Then, you train a neural network to find the best 128 dimension embeddings using the context words to predict the target word.\n",
    "\n",
    "Note: an embedding is just a fancy name for a look-up table. An embedding takes in an index (for example, 1) and returns the vector it finds at that index, which in our example would be of size 128. The embedding table should have the same number of rows as vocabulary words.\n",
    "\n",
    "And like anything involving gradient descent, our embeddings will basically start off with random numbers and we will use gradient descent to optimize them given our training data.\n",
    "\n",
    "Let's see some code motiviated by the following (also see p.407 of hands on machine learning):\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "https://github.com/FraLotito/continuous-bag-of-words\n",
    "\n",
    "## CBOW Pytorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import torch.distributions as distributions\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(tweets):\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet.split(\" \")\n",
    "        for i in range(2, len(tokens) - 2):\n",
    "            context = [tokens[i - 2], tokens[i - 1],\n",
    "                       tokens[i + 1], tokens[i + 2]]\n",
    "            target = tokens[i]\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And so the robots spared humanity ... https://t.co/v7JUJQWfCv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_csv(\"../small_data/elonmusk_tweets.csv\")\n",
    "tweets = df.text.values\n",
    "tweets = [t[2:-1] for t in tweets]\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['And', 'so', 'robots', 'spared'], 'the'),\n",
       " (['so', 'the', 'spared', 'humanity'], 'robots'),\n",
       " (['the', 'robots', 'humanity', '...'], 'spared')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = make_data(tweets)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30603"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216\n"
     ]
    }
   ],
   "source": [
    "words = [w for t in tweets for w in t.split()]\n",
    "common_words = [w for w,c in Counter(words).most_common() if c >=5]\n",
    "vocab = set(common_words)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(word):\n",
    "    if word in word_to_ix:\n",
    "        return word_to_ix[word]\n",
    "    else:\n",
    "        return len(word_to_ix)\n",
    "    \n",
    "def make_context_vector(context, get_word_index):\n",
    "    idxs = [get_word_index(w) for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## view changes the size to be one row\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.log_softmax(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = Variable(torch.LongTensor([word_to_ix[word]]))\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(len(vocab)+1, 100, 128)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(cbow.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(context, target):\n",
    "    context_vector = make_context_vector(context, get_word_index)  \n",
    "    model.zero_grad()\n",
    "    log_probs = cbow(context_vector)\n",
    "    loss = criterion(log_probs, Variable(\n",
    "        torch.LongTensor([get_word_index(target)])))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-ed42e8c7b03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-7208a44f0e07>\u001b[0m in \u001b[0;36mtrain_cbow\u001b[0;34m(context, target)\u001b[0m\n\u001b[1;32m      6\u001b[0m         torch.LongTensor([get_word_index(target)])))\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_epochs + 1):\n",
    "    for context, target in data:\n",
    "        loss = train_cbow(context, target)\n",
    "        total_loss += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of RNN and LSTM using pytorch\n",
    "\n",
    "Source: http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#sphx-glr-intermediate-char-rnn-generation-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_names = []\n",
    "name_weights = []\n",
    "for year in range(2000,2017):\n",
    "    with open(\"../small_data/baby_names/yob{}.txt\".format(year), \"r\") as f:\n",
    "        for line in f:\n",
    "            columns = line.split(\",\")\n",
    "            if columns[1] == 'F':\n",
    "                female_names.append(columns[0])\n",
    "                name_weights.append(int(columns[2].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emily', 'Hannah', 'Madison', 'Ashley', 'Sarah']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326418\n"
     ]
    }
   ],
   "source": [
    "n_names = len(female_names)\n",
    "print(n_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25953, 23078, 19967, 17996, 17691]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names_weights = defaultdict(int)\n",
    "for i in range(n_names):\n",
    "    name = female_names[i]\n",
    "    weight = name_weights[i]\n",
    "    unique_names_weights[name] = unique_names_weights[name] + weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = list(unique_names_weights.keys())\n",
    "unique_weights = list(unique_names_weights.values())\n",
    "unique_probabilities = np.array(unique_weights) / sum(unique_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.result_gate = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        \n",
    "        forget_gate_value = self.sigmoid(self.forget_gate(input_combined))\n",
    "        input_gate_value = self.sigmoid(self.input_gate(input_combined))\n",
    "        tanh_gate_value = self.tanh(self.tanh_gate(input_combined))\n",
    "        output_gate_value = self.sigmoid(self.output_gate(input_combined))\n",
    "        \n",
    "        input_tanh_combined = input_gate_value * tanh_gate_value\n",
    "        \n",
    "        cell_next = cell * forget_gate_value + input_tanh_combined\n",
    "        hidden_next = self.tanh(cell_next) * output_gate_value\n",
    "        output = self.result_gate(hidden_next)\n",
    "\n",
    "        return output, hidden_next, cell_next\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = set([c.lower() for name in female_names for c in name])\n",
    "letters_to_index = {l:i for i,l in enumerate(letters)}\n",
    "index_to_letter = {i:l for l, i in letters_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letters) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li].lower()\n",
    "        tensor[li][0][letters_to_index[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [letters_to_index[line[li].lower()] for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def randomTrainingPair():\n",
    "    return np.random.choice(unique_names, p=unique_probabilities, size=1)[0]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    line = randomTrainingPair()\n",
    "    input_line_tensor = Variable(inputTensor(line))\n",
    "    target_line_tensor = Variable(targetTensor(line))\n",
    "    return input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lstm = LSTM(n_letters, 128, n_letters)\n",
    "\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=.001)\n",
    "\n",
    "def train(input_line_tensor, target_line_tensor):\n",
    "    hidden = lstm.initHidden()\n",
    "    cell = lstm.initHidden()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size()[0]):\n",
    "        output, hidden, cell = lstm(input_line_tensor[i], hidden, cell)\n",
    "        loss += criterion(output, target_line_tensor[i])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0] / input_line_tensor.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 27s (5000 25%) 2.0953\n",
      "3m 21s (10000 50%) 0.7251\n",
      "5m 16s (15000 75%) 0.8468\n",
      "7m 20s (20000 100%) 1.3765\n"
     ]
    }
   ],
   "source": [
    "n_iters = 20000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x104e02208>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAFdCAYAAABmV5W6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151ff048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter='A'):\n",
    "    input = Variable(inputTensor(start_letter))\n",
    "    hidden = lstm.initHidden()\n",
    "    cell = lstm.initHidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output, hidden, cell = lstm(input[0], hidden, cell)\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        softmax_out = sm(Variable(output.data)).data.numpy()[0]\n",
    "        topi = np.random.choice(range(len(softmax_out)), p=softmax_out)\n",
    "        if topi == n_letters - 1:\n",
    "            break\n",
    "        else:\n",
    "            letter = index_to_letter[topi]\n",
    "            output_name += letter\n",
    "        input = Variable(inputTensor(letter))\n",
    "\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elizabeth'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
