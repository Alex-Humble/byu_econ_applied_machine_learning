{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "This is one of the simpliest and easiest to understand algorithms. It can be used for both classification and regression tasks, but is more common in classification, so we will focus there. The principles, though, can be used in both cases and sklearn supports both.\n",
    "\n",
    "* need distance metric - euclidian\n",
    "* need to define k - then nearest k vote and take most common. Can weight by distance\n",
    "* assumes points close - similar\n",
    "* Easy to undertand what it is doing, but doesnt give insights into why - just that they are close\n",
    "* high k -> high bias, and visa versa\n",
    "* curse of dimensionality means can suffer in high dimensions as likely that cloest points are not much closer than average distance, which means being close doesnt mean much"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
