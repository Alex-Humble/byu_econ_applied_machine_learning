{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is another ensemble techinque like random forests. With random forests, we trained multiple decision trees independently and created independence by randomly sampling samples and features.\n",
    "\n",
    "Boosting on the other hand trains multiple models sequentially where each each model tries to improve on the areas where the previous models performed poorly. \n",
    "\n",
    "### Adaboost\n",
    "\n",
    "Adaboost works by starting with a single model. From that model we then make predictions on the **training** set. We can then see which training samples this first model got right and which it got wrong. Adaboost then trains the next model, but puts more weight on the training samples that the first model got wrong. This process continues for N number of models where N is a hyper-parameter.\n",
    "\n",
    "It is important to note that the sequential nature of boosting makes it harder to scale and parallelize relative the random forest models. That being said, work has been done with boosting methods to allow them to be more parallelizable. A very popular library that does this is [XGboost](https://github.com/dmlc/xgboost).\n",
    "\n",
    "So - at the end of your training, you now have N models where each model is trained to do better on the instances that the previous models did poorly on. You can now combine these models via a weighted voting or average method very similar to random forest except you weight each vote by how accurate the model was overall.\n",
    "\n",
    "Adaboost also had a hyper-parameter called **learning rate.** This hyper-parameter adjusts the contribution of each classifier. When decreasing the value, each new classifier makes smaller adjustments to the weights of mis-classified samples. Basically, meaning Adaboost is slower to learn per tree. Typically a lower learning rate requires more trees to perform well. This value and the number of trees can be tuned using cross-validation. \n",
    "\n",
    "#### Math\n",
    "\n",
    "So, how exactly do we do this?\n",
    "\n",
    "**Step 1**: Set all sample weights to 1/m when have m training examples\n",
    "\n",
    "**Step 2**: Train the first model\n",
    "\n",
    "**Step 3**: Calculate the weighted error of this model, which is simply the sum of the weights of the misclassified\n",
    "examples divided by the total weight of all samples. We will call this $r_j$ for the $j$th model. The best is zero and worst is 1.\n",
    "\n",
    "**Step 4**: Calculate the predictor's weight where being more accurate gets a higher weight:\n",
    "\n",
    "$$\\alpha_{j} = \\eta log \\frac{1-r_{j}}{r_j}$$\n",
    "\n",
    "If you are wrong more than right you get a negative weight and if close to random weight close to zero\n",
    "\n",
    "**Step 5**: Update the weights of your training samples where if you got it right, the weight remains the same. If you got it wrong new weight is:\n",
    "\n",
    "$$old weight * exp(\\alpha_{j})$$\n",
    "\n",
    "Then normalize all weights to sum to 1 by dividing all weights by sum of weights.\n",
    "\n",
    "You can see that a good predictor adds extra weight to its mis-classification, putting a strong emphasis on them for the next model. Also, $\\eta$ is our learning rate and can decrease the impact of a tree on weight updates by being less than 1.\n",
    "\n",
    "**Step 6**: Repeat from step 2 with next model and continue repeating until required number of models.\n",
    "\n",
    "That's it!\n",
    "\n",
    "Predictions are made by running a new sample through all the trained models, getting the most likely class (for classification) for each one, and then doing a weighted vote where the weight is the value from step 4. For regression just weighted average.\n",
    "\n",
    "Note: Almost always the model choosen for boosting is a decision tree.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "\n",
    "### Some final notes\n",
    "\n",
    "* Boosting is more likely to overfit than random forests when the number of estimators is large. Though, usually slow to overfit.\n",
    "* Typical learning rates are around 0.01 or 0.001. And small learning rates can require a large number of estimators to achieve good results.\n",
    "* The max depth of the trees controls the complexity of the model and a max depth of 1 can often work well. This results in an additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
